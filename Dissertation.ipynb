{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyZhang127/temp/blob/main/Dissertation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp9j5j46XHkj"
      },
      "source": [
        "# 1.Loading essential modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKSevFEWbcvT"
      },
      "outputs": [],
      "source": [
        "#fbprophet\n",
        "!pip install pystan~=2.14\n",
        "!pip install fbprophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLpGV5XqSfIT",
        "outputId": "58f9c8f7-879f-4eea-b10b-c33ca3f055d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "#Basic packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import time \n",
        "import seaborn as sns # Visualization\n",
        "\n",
        "\n",
        "#Fbprophet\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from fbprophet import Prophet\n",
        "\n",
        "#SVR\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#RF\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#LSTM/RNN/GRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM,GRU,SimpleRNN\n",
        "from keras.layers import Dropout\n",
        "from tensorflow import keras as ks\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import statsmodels.stats.diagnostic\n",
        "import statsmodels.api as sm\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynoMYFDILiHj"
      },
      "source": [
        "# 2.Defining necessary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85iRkMKF7wHn"
      },
      "source": [
        "## Part A: Producing missing value randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew-9tTOs1pGO"
      },
      "outputs": [],
      "source": [
        "#Producing the missing data randomly\n",
        "def missingRate_Data(df,columns,missing_rate=0):\n",
        "  Source_missing = df.sample(frac=missing_rate)\n",
        "  Source_missing_re=(Source_missing[columns] * 0).replace(0,np.nan)\n",
        "  Source_missing[columns]=Source_missing_re[columns]\n",
        "  index = Source_missing.index\n",
        "  non_missing_df=df.query('index not in @index')\n",
        "  Source_missing_final=pd.concat([non_missing_df,Source_missing])\n",
        "\n",
        "  return Source_missing_final,index,Source_missing,Source_missing,non_missing_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOUi95V_YACu"
      },
      "source": [
        "## Part B: Defining basic imputting methods (mean,mode,median)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do2cR7ArTPGw"
      },
      "outputs": [],
      "source": [
        "# Define replace methods\n",
        "\n",
        "def mean_replace(df,col):\n",
        "  mean=df.agg({col:'mean'}).head(1)[0]\n",
        "  df=df.fillna(mean)\n",
        "  return df\n",
        "\n",
        "def mode_replace(df,col):\n",
        "  mode=df[col].mode()[0]\n",
        "  df=df.fillna(mode)\n",
        "  return df\n",
        "\n",
        "def median_replace(df,col):\n",
        "  median=df.agg({col:'median'}).head(1)[0]\n",
        "  df=df.fillna(median)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqXKXvO9YhgW"
      },
      "source": [
        "## Part C: Defining Algrithm for immputation  (SVR,RF,LSTM,RNN,MLP,GCU,FBPROPHET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Y5HiQVZUkY"
      },
      "outputs": [],
      "source": [
        "class SVR(object):\n",
        "  def __init__(self, C=1.0, ker='rbf', eps=0.2, shrink=True):\n",
        "    self.C = C\n",
        "    self.ker = ker\n",
        "    self.shrink = shrink\n",
        "    self.eps = eps\n",
        "    self.build_Model()\n",
        "\n",
        "  def build_Model(self,):\n",
        "    self.model = SVR(C=self.C, cache_size=200, coef0=0.0, degree=3, eps=self.eps, gamma='auto',ker=self.ker, max_iter=-1, shrink=self.shrink, tol=0.001, verbose=False)\n",
        "    \n",
        "  def train_Model(self, train_X, train_Y):\n",
        "    self.model.fit(train_X, train_Y)\n",
        "\n",
        "  def predict(self, test_X):\n",
        "    pred = self.model.predict(test_X)\n",
        "    return pred\n",
        "\n",
        "class MLP_M(object):\n",
        "  def __init__(self, inputDim, hiddenNum, outputDim, lr):\n",
        "    self.inputDim = inputDim\n",
        "    self.hiddenNum = hiddenNum\n",
        "    self.outputDim = outputDim\n",
        "    self.opt = ks.optimizers.RMSprop(lr=lr, rho=0.9, eps=1e-06)\n",
        "    self.build_Model()\n",
        "    \n",
        "  def build_Model(self):\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Dense(self.hiddenNum, input_dim=self.inputDim, activation='relu'))\n",
        "    self.model.add(Dense(1))\n",
        "    self.model.compile(loss='mean_squared_error', optimizer=self.opt)\n",
        "    \n",
        "  def train_Model(self, train_X, train_Y, epoch, batchSize):\n",
        "    self.model.fit(train_X, train_Y, epochs=epoch, batch_size=batchSize, verbose=1)\n",
        "    \n",
        "  def predict(self, test_X):\n",
        "    pred = self.model.predict(test_X)\n",
        "    return pred\n",
        "\n",
        "class RNNs_M(object):\n",
        "  def __init__(self, inputDim, hiddenNum, outputDim, unit, lr):\n",
        "    self.inputDim = inputDim\n",
        "    self.hiddenNum = hiddenNum\n",
        "    self.outputDim = outputDim\n",
        "    self.opt =  ks.optimizers.RMSprop(lr=lr, rho=0.9, eps=1e-06)\n",
        "    self.build_Model(unit)\n",
        "\n",
        "  def build_Model(self, unit=\"GRU\"):\n",
        "    self.model = Sequential()\n",
        "    if unit == \"GRU\":\n",
        "      self.model.add(GRU(self.hiddenNum, input_shape=(None, self.inputDim)))\n",
        "    elif unit == \"LSTM\":\n",
        "      self.model.add(LSTM(self.hiddenNum, input_shape=(None, self.inputDim)))\n",
        "    elif unit == \"RNN\":\n",
        "      self.model.add(SimpleRNN(self.hiddenNum, input_shape=(None, self.inputDim)))\n",
        "      \n",
        "    self.model.add(Dense(self.outputDim))\n",
        "    self.model.compile(loss='mean_squared_error', optimizer=self.opt, metrics=[\"mean_absolute_percentage_error\"])\n",
        "\n",
        "  def train_Model(self, train_X, train_Y, epoch, batchSize):\n",
        "    self.model.fit(train_X, train_Y, epochs=epoch, batch_size=batchSize, verbose=1, validation_split=0.0)\n",
        "\n",
        "  def predict(self,test_X):\n",
        "    pred = self.model.predict(test_X)\n",
        "    return pred\n",
        "\n",
        "class ProphetModel():\n",
        "  def __init__(self,train_X,changepoint_prior_scale=0.01):\n",
        "    self.train_X=train_X\n",
        "    self.changepoint_prior_scale=changepoint_prior_scale\n",
        "\n",
        "  def train_Model(self,train_X,changepoint_prior_scale=0.01):\n",
        "    self.model = Prophet(changepoint_prior_scale).fit(train_X)\n",
        "\n",
        "  def predict(self,test_X):\n",
        "    pred = self.model.predict(test_X)\n",
        "    return pred\n",
        "\n",
        "class Random_forest():\n",
        "  def __init__(self,train_X,train_Y):\n",
        "    self.train_X=train_X\n",
        "    self.train_Y=train_Y\n",
        "\n",
        "  def train_Model(self,train_X,train_Y):\n",
        "    self.rf=RandomForestRegressor()\n",
        "    self.model=self.rf.fit(train_X, train_Y)\n",
        "\n",
        "  def predict(self,test_X):\n",
        "    pred = self.rf.predict(test_X)\n",
        "    return pred\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URve0tVRZOLl"
      },
      "source": [
        "## Part D: Defining validation methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I61kguRmPlWX"
      },
      "outputs": [],
      "source": [
        "# Define validation methods\n",
        "\n",
        "def MSE(actual,predicted):\n",
        "  MSE = np.square(np.subtract(actual,predicted)).mean() \n",
        "  MSE = math.sqrt(MSE)\n",
        "  return MSE\n",
        "\n",
        "def RMSE(actual,predicted):\n",
        "  MSE = np.square(np.subtract(actual,predicted)).mean() \n",
        "  MSE = math.sqrt(MSE)\n",
        "  RMSE = math.sqrt(MSE)\n",
        "  return RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTlNc2qTNfQe"
      },
      "source": [
        "# 3.Obtain Source Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV3_YzwSNmhZ"
      },
      "outputs": [],
      "source": [
        "source_file=pd.read_csv('/content/Occupancy_Estimation.csv')\n",
        "source_file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_file.groupby('Date').count()"
      ],
      "metadata": {
        "id": "ENhdsMv7W2rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_BlqkqKNscv"
      },
      "outputs": [],
      "source": [
        "source_file['Datetime']=pd.to_datetime(source_file['Date']+' '+source_file['Time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hL5iH0yNyWx"
      },
      "outputs": [],
      "source": [
        "source_continues_data=source_file[(source_file.Date >= '2017/12/22') & (source_file.Date <= '2017/12/24')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlfu2N6cN27t"
      },
      "outputs": [],
      "source": [
        "source_continues_data=source_continues_data[['Datetime','S1_Temp','S2_Temp','S3_Temp','S4_Temp','S1_Light','S2_Light','S3_Light','S4_Light','S1_Sound','S2_Sound','S3_Sound','S4_Sound']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_cll-trN4W0"
      },
      "outputs": [],
      "source": [
        "source_continues_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egyqgQf9OM8G"
      },
      "outputs": [],
      "source": [
        "source_continues_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfNCiXasMxhg"
      },
      "source": [
        "# 4.Visualize source data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuKPA55cNYhH"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(25, 10),facecolor='w')\n",
        "axs[0,0].plot(source_continues_data['Datetime'], source_continues_data['S1_Temp'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "axs[0,1].plot(source_continues_data['Datetime'], source_continues_data['S2_Temp'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "axs[1,0].plot(source_continues_data['Datetime'], source_continues_data['S3_Temp'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "axs[1,1].plot(source_continues_data['Datetime'], source_continues_data['S4_Temp'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "fig.suptitle('Temperature')\n",
        "\n",
        "axs[0,0].grid(False)\n",
        "axs[0,1].grid(False)\n",
        "axs[1,0].grid(False)\n",
        "axs[1,1].grid(False)\n",
        "\n",
        "axs[0,0].set_facecolor('white')\n",
        "axs[0,1].set_facecolor('white')\n",
        "axs[1,0].set_facecolor('white')\n",
        "axs[1,1].set_facecolor('white')\n",
        "\n",
        "\n",
        "axs[0,0].set_title('S1_Temp', fontsize=14)\n",
        "axs[0,1].set_title('S2_Temp', fontsize=14)\n",
        "axs[1,0].set_title('S3_Temp', fontsize=14)\n",
        "axs[1,1].set_title('S4_Temp', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0hQ49mGNXbI"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(25, 10),facecolor='w')\n",
        "axs[0,0].plot(source_continues_data['Datetime'], source_continues_data['S1_Light'],color='darkorange', label='original', linewidth=1)\n",
        "axs[0,1].plot(source_continues_data['Datetime'], source_continues_data['S2_Light'],color='darkorange', label='original', linewidth=1)\n",
        "axs[1,0].plot(source_continues_data['Datetime'], source_continues_data['S3_Light'],color='darkorange', label='original', linewidth=1)\n",
        "axs[1,1].plot(source_continues_data['Datetime'], source_continues_data['S4_Light'],color='darkorange', label='original', linewidth=1)\n",
        "fig.suptitle('Light')\n",
        "\n",
        "axs[0,0].grid(False)\n",
        "axs[0,1].grid(False)\n",
        "axs[1,0].grid(False)\n",
        "axs[1,1].grid(False)\n",
        "\n",
        "axs[0,0].set_facecolor('white')\n",
        "axs[0,1].set_facecolor('white')\n",
        "axs[1,0].set_facecolor('white')\n",
        "axs[1,1].set_facecolor('white')\n",
        "\n",
        "\n",
        "axs[0,0].set_title('S1_Light', fontsize=14)\n",
        "axs[0,1].set_title('S2_Light', fontsize=14)\n",
        "axs[1,0].set_title('S3_Light', fontsize=14)\n",
        "axs[1,1].set_title('S4_Light', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_7909ipNNbP"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(25, 10),facecolor='w')\n",
        "axs[0,0].plot(source_continues_data['Datetime'], source_continues_data['S1_Sound'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "axs[0,1].plot(source_continues_data['Datetime'], source_continues_data['S2_Sound'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "axs[1,0].plot(source_continues_data['Datetime'], source_continues_data['S3_Sound'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "axs[1,1].plot(source_continues_data['Datetime'], source_continues_data['S4_Sound'],color='darkorange', label='original', linewidth=2,markerfacecolor='white')\n",
        "fig.suptitle('Sound')\n",
        "\n",
        "axs[0,0].grid(False)\n",
        "axs[0,1].grid(False)\n",
        "axs[1,0].grid(False)\n",
        "axs[1,1].grid(False)\n",
        "\n",
        "axs[0,0].set_facecolor('white')\n",
        "axs[0,1].set_facecolor('white')\n",
        "axs[1,0].set_facecolor('white')\n",
        "axs[1,1].set_facecolor('white')\n",
        "\n",
        "\n",
        "axs[0,0].set_title('S1_Sound', fontsize=14)\n",
        "axs[0,1].set_title('S2_Sound', fontsize=14)\n",
        "axs[1,0].set_title('S3_Sound', fontsize=14)\n",
        "axs[1,1].set_title('S4_Sound', fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMDdlESRgvd"
      },
      "source": [
        "# 5.Genenrate missing value randamly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yg9TA1MWo6K"
      },
      "outputs": [],
      "source": [
        "columns=['S1_Temp','S2_Temp','S3_Temp','S4_Temp','S1_Light','S2_Light','S3_Light','S4_Light','S1_Sound','S2_Sound','S3_Sound','S4_Sound']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU6rWji6S1mT"
      },
      "outputs": [],
      "source": [
        "source_continues_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAHGyCQeWnaf"
      },
      "outputs": [],
      "source": [
        "Source_missing_final_1,index_1,Source_missing_1,Source_missing_1,non_missing_df_1=missingRate_Data(source_continues_data,columns,missing_rate=0.05)\n",
        "Source_missing_final_2,index_2,Source_missing_2,Source_missing_2,non_missing_df_2=missingRate_Data(source_continues_data,columns,missing_rate=0.1)\n",
        "Source_missing_final_3,index_3,Source_missing_3,Source_missing_3,non_missing_df_3=missingRate_Data(source_continues_data,columns,missing_rate=0.15)\n",
        "Source_missing_final_4,index_4,Source_missing_4,Source_missing_4,non_missing_df_4=missingRate_Data(source_continues_data,columns,missing_rate=0.20)\n",
        "Source_missing_final_5,index_5,Source_missing_5,Source_missing_5,non_missing_df_5=missingRate_Data(source_continues_data,columns,missing_rate=0.25)\n",
        "Source_missing_final_6,index_6,Source_missing_6,Source_missing_6,non_missing_df_6=missingRate_Data(source_continues_data,columns,missing_rate=0.30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEMCZ5DXYm0t"
      },
      "outputs": [],
      "source": [
        "Source_missing_final_1_1=Source_missing_final_1.sort_index()\n",
        "Source_missing_final_2_1=Source_missing_final_2.sort_index()\n",
        "Source_missing_final_3_1=Source_missing_final_3.sort_index()\n",
        "Source_missing_final_4_1=Source_missing_final_4.sort_index()\n",
        "Source_missing_final_5_1=Source_missing_final_5.sort_index()\n",
        "Source_missing_final_6_1=Source_missing_final_6.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSfQ1gPznPGj"
      },
      "outputs": [],
      "source": [
        "Source_missing_final_1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZRMa8-mnIR4"
      },
      "outputs": [],
      "source": [
        "Source_missing_final_1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI5n_XhuZnAN"
      },
      "outputs": [],
      "source": [
        "import missingno as mno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3knnZXTPa5zb"
      },
      "outputs": [],
      "source": [
        "mno.matrix(Source_missing_final_1_1, figsize = (20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3tKrI82bA4N"
      },
      "outputs": [],
      "source": [
        "mno.matrix(Source_missing_final_2_1, figsize = (20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liStwf55bEnd"
      },
      "outputs": [],
      "source": [
        "mno.matrix(Source_missing_final_3_1, figsize = (20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo7YTKbibMXi"
      },
      "outputs": [],
      "source": [
        "mno.matrix(Source_missing_final_4_1, figsize = (20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTg0xqXcbRLk"
      },
      "outputs": [],
      "source": [
        "mno.matrix(Source_missing_final_5_1, figsize = (20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QfJ9jYMbUmG"
      },
      "outputs": [],
      "source": [
        "mno.matrix(Source_missing_final_6_1, figsize = (20, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHZqR-xM77ih"
      },
      "source": [
        "# 6.Deploy different approches on datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3DWh6bsfsTc"
      },
      "outputs": [],
      "source": [
        "columns=source_continues_data.drop(['Datetime'], axis=1).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMW5BWkWicTk",
        "outputId": "7ae1ddd6-2985-45e8-804c-bf458c4062dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp', 'S1_Light', 'S2_Light',\n",
              "       'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0TTb7Ozig_n"
      },
      "outputs": [],
      "source": [
        "source_continues_data[['S1_Temp']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZZF1U4lg-io"
      },
      "outputs": [],
      "source": [
        "mean_5={}\n",
        "\n",
        "for col in columns:\n",
        "  df=mean_replace(Source_missing_final_1[[col]],col)\n",
        "  mean_5[col]=df\n",
        "\n",
        "mean_10={}\n",
        "for col in columns:\n",
        "  df=mean_replace(Source_missing_final_2[[col]],col)\n",
        "  mean_10[col]=df\n",
        "\n",
        "mean_15={}\n",
        "for col in columns:\n",
        "  df=mean_replace(Source_missing_final_3[[col]],col)\n",
        "  mean_15[col]=df\n",
        "\n",
        "mean_20={}\n",
        "for col in columns:\n",
        "  df=mean_replace(Source_missing_final_4[[col]],col)\n",
        "  mean_20[col]=df\n",
        "\n",
        "mean_25={}\n",
        "for col in columns:\n",
        "  df=mean_replace(Source_missing_final_5[[col]],col)\n",
        "  mean_25[col]=df\n",
        "\n",
        "mean_30={}\n",
        "for col in columns:\n",
        "  df=mean_replace(Source_missing_final_6[[col]],col)\n",
        "  mean_30[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GjQaWHqjm9o"
      },
      "outputs": [],
      "source": [
        "median_5={}\n",
        "\n",
        "for col in columns:\n",
        "  df=median_replace(Source_missing_final_1[[col]],col)\n",
        "  median_5[col]=df\n",
        "\n",
        "median_10={}\n",
        "for col in columns:\n",
        "  df=median_replace(Source_missing_final_2[[col]],col)\n",
        "  median_10[col]=df\n",
        "\n",
        "median_15={}\n",
        "for col in columns:\n",
        "  df=median_replace(Source_missing_final_3[[col]],col)\n",
        "  median_15[col]=df\n",
        "\n",
        "median_20={}\n",
        "for col in columns:\n",
        "  df=median_replace(Source_missing_final_4[[col]],col)\n",
        "  median_20[col]=df\n",
        "\n",
        "median_25={}\n",
        "for col in columns:\n",
        "  df=median_replace(Source_missing_final_5[[col]],col)\n",
        "  median_25[col]=df\n",
        "\n",
        "median_30={}\n",
        "for col in columns:\n",
        "  df=median_replace(Source_missing_final_6[[col]],col)\n",
        "  median_30[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FISPUYyIj44f"
      },
      "outputs": [],
      "source": [
        "mode_5={}\n",
        "\n",
        "for col in columns:\n",
        "  df=mode_replace(Source_missing_final_1[[col]],col)\n",
        "  mode_5[col]=df\n",
        "\n",
        "mode_10={}\n",
        "for col in columns:\n",
        "  df=mode_replace(Source_missing_final_2[[col]],col)\n",
        "  mode_10[col]=df\n",
        "\n",
        "mode_15={}\n",
        "for col in columns:\n",
        "  df=mode_replace(Source_missing_final_3[[col]],col)\n",
        "  mode_15[col]=df\n",
        "\n",
        "mode_20={}\n",
        "for col in columns:\n",
        "  df=mode_replace(Source_missing_final_4[[col]],col)\n",
        "  mode_20[col]=df\n",
        "\n",
        "mode_25={}\n",
        "for col in columns:\n",
        "  df=mode_replace(Source_missing_final_5[[col]],col)\n",
        "  mode_25[col]=df\n",
        "\n",
        "mode_30={}\n",
        "for col in columns:\n",
        "  df=mode_replace(Source_missing_final_6[[col]],col)\n",
        "  mode_30[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqLjea-jqX0k"
      },
      "outputs": [],
      "source": [
        "#list1=[mode_5,mode_10,mode_15,mode_20,mode_25,mode_30\n",
        "#,median_5,median_10,median_15,median_20,median_25,median_30\n",
        "#,mean_5,mean_10,mean_15,mean_20,mean_25,mean_30]\n",
        "\n",
        "list1=[mean_5,mean_10,mean_15,mean_20,mean_25,mean_30]\n",
        "\n",
        "\n",
        "col_MSE={}\n",
        "\n",
        "for col in columns:\n",
        "  for md in list1:\n",
        "    col_MSE=MSE(source_continues_data[[col]],md[col])\n",
        "    print(col_MSE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHPR0SDQzJmo"
      },
      "outputs": [],
      "source": [
        "col_RMSE={}\n",
        "\n",
        "for md in list1:\n",
        "  for col in columns:\n",
        "    col_RMSE=RMSE(source_continues_data[[col]],md[col])\n",
        "    print(col_RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frhcp-zvMVBN"
      },
      "source": [
        "## 6.1 Prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxEQYIDieQPG"
      },
      "outputs": [],
      "source": [
        "def get_format(df,col):\n",
        "  df=df[['Datetime',col]]\n",
        "  df['ds']=df['Datetime']\n",
        "  df['y']=df[col]\n",
        "  df=df[['ds','y']]\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aedcrK2LZg9j"
      },
      "outputs": [],
      "source": [
        "def run_Prophet(train_Model,future):\n",
        "  Prm=ProphetModel(train_Model)\n",
        "  Prm = Prophet(changepoint_prior_scale=0.01).fit(train_Model)\n",
        "  Pred = Prm.predict(future)\n",
        "  Pred_1=Pred[['ds','yhat']]\n",
        "  Pred_1['y']=Pred['yhat']\n",
        "  Pred_1=Pred_1[['ds','y']]\n",
        "  pred_1_comp=pd.concat([train_Model,Pred_1])\n",
        "  return pred_1_comp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo_zA1tRh2MG"
      },
      "outputs": [],
      "source": [
        "columns=source_continues_data.drop(['Datetime'], axis=1).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFOd2ABDhWrh"
      },
      "outputs": [],
      "source": [
        "index=index_1\n",
        "Prophet_predict_1={}\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  Prophet_df=get_format(source_continues_data,col)\n",
        "  future_1=Prophet_df.iloc[index,:][['ds']]\n",
        "  train_Model_1=Prophet_df.query('index not in @index')\n",
        "  Prophet_pred_df=run_Prophet(train_Model_1,future_1)\n",
        "  Prophet_predict_1[col]=Prophet_pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA_iQtsfj5AP"
      },
      "outputs": [],
      "source": [
        "index=index_2\n",
        "Prophet_predict_2={}\n",
        "\n",
        "for col in columns:\n",
        "  Prophet_df=get_format(source_continues_data,col)\n",
        "  future_1=Prophet_df.iloc[index,:][['ds']]\n",
        "  train_Model_1=Prophet_df.query('index not in @index')\n",
        "  Prophet_pred_df=run_Prophet(train_Model_1,future_1)\n",
        "  Prophet_predict_2[col]=Prophet_pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF6xpCXHj-Jx"
      },
      "outputs": [],
      "source": [
        "index=index_3\n",
        "Prophet_predict_3={}\n",
        "\n",
        "for col in columns:\n",
        "  Prophet_df=get_format(source_continues_data,col)\n",
        "  future_1=Prophet_df.iloc[index,:][['ds']]\n",
        "  train_Model_1=Prophet_df.query('index not in @index')\n",
        "  Prophet_pred_df=run_Prophet(train_Model_1,future_1)\n",
        "  Prophet_predict_3[col]=Prophet_pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsqaKeackyzh"
      },
      "outputs": [],
      "source": [
        "index=index_4\n",
        "Prophet_predict_4={}\n",
        "\n",
        "for col in columns:\n",
        "  Prophet_df=get_format(source_continues_data,col)\n",
        "  future_1=Prophet_df.iloc[index,:][['ds']]\n",
        "  train_Model_1=Prophet_df.query('index not in @index')\n",
        "  Prophet_pred_df=run_Prophet(train_Model_1,future_1)\n",
        "  Prophet_predict_4[col]=Prophet_pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvEb-S3XlG87"
      },
      "outputs": [],
      "source": [
        "index=index_5\n",
        "Prophet_predict_5={}\n",
        "\n",
        "for col in columns:\n",
        "  Prophet_df=get_format(source_continues_data,col)\n",
        "  future_1=Prophet_df.iloc[index,:][['ds']]\n",
        "  train_Model_1=Prophet_df.query('index not in @index')\n",
        "  Prophet_pred_df=run_Prophet(train_Model_1,future_1)\n",
        "  Prophet_predict_5[col]=Prophet_pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSAKm5KnvCN"
      },
      "outputs": [],
      "source": [
        "index=index_6\n",
        "Prophet_predict_6={}\n",
        "\n",
        "for col in columns:\n",
        "  Prophet_df=get_format(source_continues_data,col)\n",
        "  future_1=Prophet_df.iloc[index,:][['ds']]\n",
        "  train_Model_1=Prophet_df.query('index not in @index')\n",
        "  Prophet_pred_df=run_Prophet(train_Model_1,future_1)\n",
        "  Prophet_predict_6[col]=Prophet_pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LhAjWoDMaoE"
      },
      "outputs": [],
      "source": [
        "list1=[Prophet_predict_1,Prophet_predict_2,Prophet_predict_3,Prophet_predict_4,Prophet_predict_5,Prophet_predict_6]\n",
        "\n",
        "\n",
        "col_MSE={}\n",
        "\n",
        "for col in columns:\n",
        "  for md in list1:\n",
        "    col_MSE=MSE(source_continues_data[[col]],md[col][['y']].sort_index())\n",
        "    print(col_MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXob9DcX_Oiq"
      },
      "outputs": [],
      "source": [
        "list1=[Prophet_predict_1,Prophet_predict_2,Prophet_predict_3,Prophet_predict_4,Prophet_predict_5,Prophet_predict_6]\n",
        "\n",
        "\n",
        "col_MSE={}\n",
        "\n",
        "for col in columns:\n",
        "  for md in list1:\n",
        "    col_MSE=MSE(source_continues_data[[col]],md[col][['y']].sort_index())\n",
        "    print(col_MSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yutws9-TtfWf"
      },
      "source": [
        "## 6.2 RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l49qJ-cg_MR3"
      },
      "outputs": [],
      "source": [
        "index=index_1\n",
        "RF_predict_1={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  rf=RandomForestRegressor()\n",
        "  model = rf.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rf.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  RF_predict_1[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS8oFUfKCG8D"
      },
      "outputs": [],
      "source": [
        "index=index_2\n",
        "RF_predict_2={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "for col in columns:\n",
        "  rf=RandomForestRegressor()\n",
        "  model = rf.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rf.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  RF_predict_2[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gdyN4yvCVww"
      },
      "outputs": [],
      "source": [
        "index=index_3\n",
        "RF_predict_3={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "for col in columns:\n",
        "  rf=RandomForestRegressor()\n",
        "  model = rf.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rf.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  RF_predict_3[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhyEYkXHCYay"
      },
      "outputs": [],
      "source": [
        "index=index_4\n",
        "RF_predict_4={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "for col in columns:\n",
        "  rf=RandomForestRegressor()\n",
        "  model = rf.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rf.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  RF_predict_4[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTeb2tdPCcGB"
      },
      "outputs": [],
      "source": [
        "index=index_5\n",
        "RF_predict_5={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "for col in columns:\n",
        "  rf=RandomForestRegressor()\n",
        "  model = rf.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rf.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  RF_predict_5[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKA6MLfzCfMj"
      },
      "outputs": [],
      "source": [
        "index=index_6\n",
        "RF_predict_6={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "for col in columns:\n",
        "  rf=RandomForestRegressor()\n",
        "  model = rf.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rf.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  RF_predict_6[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxGFmh-hMljH"
      },
      "outputs": [],
      "source": [
        "list1=[RF_predict_1,RF_predict_2,RF_predict_3,RF_predict_4,RF_predict_5,RF_predict_6]\n",
        "\n",
        "\n",
        "col_MSE={}\n",
        "\n",
        "for col in columns:\n",
        "  for md in list1:\n",
        "    col_MSE=MSE(source_continues_data[[col]],md[col].sort_index())\n",
        "    print(col_MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDXX5ByyleNp"
      },
      "outputs": [],
      "source": [
        "col_RMSE={}\n",
        "\n",
        "for md in list1:\n",
        "  for col in columns:\n",
        "    col_RMSE=RMSE(source_continues_data[[col]],md[col].sort_index())\n",
        "    print(col_RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKStzwmHuhC_"
      },
      "source": [
        "## 6.3 SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vys-acN4JwLy"
      },
      "outputs": [],
      "source": [
        "index=index_1\n",
        "SVC_predict_1={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  rbf_svr = SVR(ker = 'rbf', C = 1000.0, gamma = 0.1)\n",
        "  rbf_svr.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rbf_svr.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  SVC_predict_1[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beSb3a93KiZ3"
      },
      "outputs": [],
      "source": [
        "index=index_2\n",
        "SVC_predict_2={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  rbf_svr = SVR(ker = 'rbf', C = 1000.0, gamma = 0.1)\n",
        "  rbf_svr.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rbf_svr.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  SVC_predict_2[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i5dwbcuKjZV"
      },
      "outputs": [],
      "source": [
        "index=index_3\n",
        "SVC_predict_3={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  rbf_svr = SVR(ker = 'rbf', C = 1000.0, gamma = 0.1)\n",
        "  rbf_svr.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rbf_svr.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  SVC_predict_3[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzJDKsT2KlRq"
      },
      "outputs": [],
      "source": [
        "index=index_4\n",
        "SVC_predict_4={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  rbf_svr = SVR(ker = 'rbf', C = 1000.0, gamma = 0.1)\n",
        "  rbf_svr.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rbf_svr.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  SVC_predict_4[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zgDs9g2Kl0U"
      },
      "outputs": [],
      "source": [
        "index=index_5\n",
        "SVC_predict_5={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  rbf_svr = SVR(ker = 'rbf', C = 1000.0, gamma = 0.1)\n",
        "  rbf_svr.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rbf_svr.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  SVC_predict_5[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElhBqkOBLc1Q"
      },
      "outputs": [],
      "source": [
        "index=index_6\n",
        "SVC_predict_6={}\n",
        "\n",
        "test_df=source_continues_data.iloc[index,:]\n",
        "train_Model_df=source_continues_data.query('index not in @index')\n",
        "\n",
        "train_Model_x=[]\n",
        "test_x=[]\n",
        "for x in train_Model_df['Datetime'].values:\n",
        "  train_Model_x.append([x])\n",
        "for x in test_df['Datetime'].values:\n",
        "  test_x.append([x])\n",
        "\n",
        "\n",
        "for col in columns:\n",
        "  rbf_svr = SVR(ker = 'rbf', C = 1000.0, gamma = 0.1)\n",
        "  rbf_svr.fit(train_Model_x, train_Model_df[col].values)\n",
        "  y_tests_predict=rbf_svr.predict(test_x)\n",
        "  train_Model_df_tmp=train_Model_df[[col]]\n",
        "  test_df_tmp=test_df[[col]]\n",
        "  test_df_tmp['predict']=y_tests_predict\n",
        "  train_Model_df_tmp['predict']=train_Model_df_tmp[col]\n",
        "  SVC_predict_6[col]=pd.concat([train_Model_df_tmp[['predict']],test_df_tmp[['predict']]]).rename(columns={'predict':col})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3If3mVWuVA_b"
      },
      "outputs": [],
      "source": [
        "list1=[SVC_predict_1,SVC_predict_2,SVC_predict_3,SVC_predict_4,SVC_predict_5,SVC_predict_6]\n",
        "\n",
        "\n",
        "col_MSE={}\n",
        "\n",
        "for col in columns:\n",
        "  for md in list1:\n",
        "    col_MSE=MSE(source_continues_data[[col]],md[col].sort_index())\n",
        "    print(col_MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5qqvDRNVA_c"
      },
      "outputs": [],
      "source": [
        "col_RMSE={}\n",
        "\n",
        "for md in list1:\n",
        "  for col in columns:\n",
        "    col_RMSE=RMSE(source_continues_data[[col]],md[col].sort_index())\n",
        "    print(col_RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3o1nnCNpn_n"
      },
      "source": [
        "##6.4 MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjPCEpU1d494"
      },
      "outputs": [],
      "source": [
        "#normalize time series\n",
        "def NormalizeDataset(dataset):\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  dataset = scaler.fit_transform(dataset)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu1I16aNxsno"
      },
      "outputs": [],
      "source": [
        "#dividetrain_ModelTest\n",
        "def dividetrain_ModelTest(dataset, index):\n",
        "  train_Model,test=[],[]\n",
        "  for i in range(len(dataset)):\n",
        "    if i in index:\n",
        "      test.append([dataset[i][0]])\n",
        "    else:\n",
        "      train_Model.append([dataset[i][0]])\n",
        "\n",
        "  return np.asarray(train_Model), np.asarray(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l-oR_AtfPLi"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "def load_data(dataset,col):\n",
        "  ts = dataset[col]\n",
        "  data = ts.values.reshape(-1, 1).astype(\"float32\")\n",
        "  return ts,data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62NuSANOooRt"
      },
      "outputs": [],
      "source": [
        "def createSamples(dataset, lookBack, RNN=True):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset) - lookBack):\n",
        "        sample_X = dataset[i:(i + lookBack), :]\n",
        "        sample_Y = dataset[i + lookBack, :]\n",
        "        dataX.append(sample_X)\n",
        "        dataY.append(sample_Y)\n",
        "    dataX = np.array(dataX) \n",
        "    dataY = np.array(dataY)\n",
        "    if not RNN:\n",
        "        dataX = np.reshape(dataX, (dataX.shape[0], dataX.shape[1]))\n",
        "\n",
        "    return dataX, dataY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X4hnnf0Thch"
      },
      "outputs": [],
      "source": [
        "from numpy.ma.core import append\n",
        "array=[]\n",
        "\n",
        "for x in range(len(source_continues_data)):\n",
        "  array.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z5xfAGiL25R"
      },
      "outputs": [],
      "source": [
        "hiddenNum=64\n",
        "lr=1e-4\n",
        "epoch=20\n",
        "batchSize=32\n",
        "plot_flag=True\n",
        "outputDim=1\n",
        "\n",
        "index=index_1\n",
        "MLPm_index_1={}\n",
        "for col in columns:\n",
        "  Source_missing_final_1['index']=array\n",
        "  Source_missing_final_1=Source_missing_final_1.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_1[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  inputDim=lookBack\n",
        "  train_X_n, trainY_n = createSamples(train_Model, lookBack, RNN=False)\n",
        "  test_X_n, testY_n = createSamples(test, lookBack, RNN=False)\n",
        "  MLPm=MLP_M(inputDim, hiddenNum, outputDim, lr)\n",
        "  MLPm.build_Model()\n",
        "  MLPm.train_Model(train_X_n, trainY_n, epoch, batchSize)\n",
        "  MLPm_train_ModelPred = MLPm.predict(train_X_n)\n",
        "  MLPm_testPred = MLPm.predict(test_X_n)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  MLPm_testPred = scaler.inverse_transform(MLPm_testPred)\n",
        "  MLPm_testPred=pd.DataFrame(MLPm_testPred).rename(columns={0: col})\n",
        "  MLPm_testPred=MLPm_testPred.iloc[len(MLPm_testPred)-len(index):,:].set_index(index)\n",
        "  MLPM_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([MLPM_pre, MLPm_testPred]).sort_index()\n",
        "  MLPm_index_1[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JdoJ3dTgzwq"
      },
      "outputs": [],
      "source": [
        "hiddenNum=64\n",
        "lr=1e-4\n",
        "epoch=20\n",
        "batchSize=32\n",
        "plot_flag=True\n",
        "outputDim=1\n",
        "\n",
        "index=index_2\n",
        "MLPm_index_2={}\n",
        "for col in columns:\n",
        "  Source_missing_final_2['index']=array\n",
        "  Source_missing_final_2=Source_missing_final_2.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_2[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  inputDim=lookBack\n",
        "  train_X_n, trainY_n = createSamples(train_Model, lookBack, RNN=False)\n",
        "  test_X_n, testY_n = createSamples(test, lookBack, RNN=False)\n",
        "  MLPm=MLP_M(inputDim, hiddenNum, outputDim, lr)\n",
        "  MLPm.build_Model()\n",
        "  MLPm.train_Model(train_X_n, trainY_n, epoch, batchSize)\n",
        "  MLPm_train_ModelPred = MLPm.predict(train_X_n)\n",
        "  MLPm_testPred = MLPm.predict(test_X_n)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  MLPm_testPred = scaler.inverse_transform(MLPm_testPred)\n",
        "  MLPm_testPred=pd.DataFrame(MLPm_testPred).rename(columns={0: col})\n",
        "  MLPm_testPred=MLPm_testPred.iloc[len(MLPm_testPred)-len(index):,:].set_index(index)\n",
        "  MLPM_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([MLPM_pre, MLPm_testPred]).sort_index()\n",
        "  MLPm_index_2[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-FkfMX9g-gg"
      },
      "outputs": [],
      "source": [
        "hiddenNum=64\n",
        "lr=1e-4\n",
        "epoch=20\n",
        "batchSize=32\n",
        "plot_flag=True\n",
        "outputDim=1\n",
        "\n",
        "index=index_3\n",
        "MLPm_index_3={}\n",
        "for col in columns:\n",
        "  Source_missing_final_3['index']=array\n",
        "  Source_missing_final_3=Source_missing_final_3.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_3[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  inputDim=lookBack\n",
        "  train_X_n, trainY_n = createSamples(train_Model, lookBack, RNN=False)\n",
        "  test_X_n, testY_n = createSamples(test, lookBack, RNN=False)\n",
        "  MLPm=MLP_M(inputDim, hiddenNum, outputDim, lr)\n",
        "  MLPm.build_Model()\n",
        "  MLPm.train_Model(train_X_n, trainY_n, epoch, batchSize)\n",
        "  MLPm_train_ModelPred = MLPm.predict(train_X_n)\n",
        "  MLPm_testPred = MLPm.predict(test_X_n)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  MLPm_testPred = scaler.inverse_transform(MLPm_testPred)\n",
        "  MLPm_testPred=pd.DataFrame(MLPm_testPred).rename(columns={0: col})\n",
        "  MLPm_testPred=MLPm_testPred.iloc[len(MLPm_testPred)-len(index):,:].set_index(index)\n",
        "  MLPM_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([MLPM_pre, MLPm_testPred]).sort_index()\n",
        "  MLPm_index_3[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBcciMJ7hGfa"
      },
      "outputs": [],
      "source": [
        "hiddenNum=64\n",
        "lr=1e-4\n",
        "epoch=20\n",
        "batchSize=32\n",
        "plot_flag=True\n",
        "outputDim=1\n",
        "\n",
        "index=index_4\n",
        "MLPm_index_4={}\n",
        "for col in columns:\n",
        "  Source_missing_final_4['index']=array\n",
        "  Source_missing_final_4=Source_missing_final_4.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_4[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  inputDim=lookBack\n",
        "  train_X_n, trainY_n = createSamples(train_Model, lookBack, RNN=False)\n",
        "  test_X_n, testY_n = createSamples(test, lookBack, RNN=False)\n",
        "  MLPm=MLP_M(inputDim, hiddenNum, outputDim, lr)\n",
        "  MLPm.build_Model()\n",
        "  MLPm.train_Model(train_X_n, trainY_n, epoch, batchSize)\n",
        "  MLPm_train_ModelPred = MLPm.predict(train_X_n)\n",
        "  MLPm_testPred = MLPm.predict(test_X_n)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  MLPm_testPred = scaler.inverse_transform(MLPm_testPred)\n",
        "  MLPm_testPred=pd.DataFrame(MLPm_testPred).rename(columns={0: col})\n",
        "  MLPm_testPred=MLPm_testPred.iloc[len(MLPm_testPred)-len(index):,:].set_index(index)\n",
        "  MLPM_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([MLPM_pre, MLPm_testPred]).sort_index()\n",
        "  MLPm_index_4[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arb05fMGhOT_"
      },
      "outputs": [],
      "source": [
        "hiddenNum=64\n",
        "lr=1e-4\n",
        "epoch=20\n",
        "batchSize=32\n",
        "plot_flag=True\n",
        "outputDim=1\n",
        "\n",
        "index=index_5\n",
        "MLPm_index_5={}\n",
        "for col in columns:\n",
        "  Source_missing_final_5['index']=array\n",
        "  Source_missing_final_5=Source_missing_final_5.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_5[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  inputDim=lookBack\n",
        "  train_X_n, trainY_n = createSamples(train_Model, lookBack, RNN=False)\n",
        "  test_X_n, testY_n = createSamples(test, lookBack, RNN=False)\n",
        "  MLPm=MLP_M(inputDim, hiddenNum, outputDim, lr)\n",
        "  MLPm.build_Model()\n",
        "  MLPm.train_Model(train_X_n, trainY_n, epoch, batchSize)\n",
        "  MLPm_train_ModelPred = MLPm.predict(train_X_n)\n",
        "  MLPm_testPred = MLPm.predict(test_X_n)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  MLPm_testPred = scaler.inverse_transform(MLPm_testPred)\n",
        "  MLPm_testPred=pd.DataFrame(MLPm_testPred).rename(columns={0: col})\n",
        "  MLPm_testPred=MLPm_testPred.iloc[len(MLPm_testPred)-len(index):,:].set_index(index)\n",
        "  MLPM_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([MLPM_pre, MLPm_testPred]).sort_index()\n",
        "  MLPm_index_5[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXo4dSIWhVD9"
      },
      "outputs": [],
      "source": [
        "hiddenNum=64\n",
        "lr=1e-4\n",
        "epoch=20\n",
        "batchSize=32\n",
        "plot_flag=True\n",
        "outputDim=1\n",
        "\n",
        "index=index_6\n",
        "MLPm_index_6={}\n",
        "for col in columns:\n",
        "  Source_missing_final_6['index']=array\n",
        "  Source_missing_final_6=Source_missing_final_6.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_6[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  inputDim=lookBack\n",
        "  train_X_n, trainY_n = createSamples(train_Model, lookBack, RNN=False)\n",
        "  test_X_n, testY_n = createSamples(test, lookBack, RNN=False)\n",
        "  MLPm=MLP_M(inputDim, hiddenNum, outputDim, lr)\n",
        "  MLPm.build_Model()\n",
        "  MLPm.train_Model(train_X_n, trainY_n, epoch, batchSize)\n",
        "  MLPm_train_ModelPred = MLPm.predict(train_X_n)\n",
        "  MLPm_testPred = MLPm.predict(test_X_n)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  MLPm_testPred = scaler.inverse_transform(MLPm_testPred)\n",
        "  MLPm_testPred=pd.DataFrame(MLPm_testPred).rename(columns={0: col})\n",
        "  MLPm_testPred=MLPm_testPred.iloc[len(MLPm_testPred)-len(index):,:].set_index(index)\n",
        "  MLPM_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([MLPM_pre, MLPm_testPred]).sort_index()\n",
        "  MLPm_index_6[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ3lOqnOArDo"
      },
      "outputs": [],
      "source": [
        "indexs=[MLPm_index_1,MLPm_index_2,MLPm_index_3,MLPm_index_4,MLPm_index_5,MLPm_index_6]\n",
        "\n",
        "for col in columns:\n",
        "  #print(col)\n",
        "  for index in indexs:\n",
        "    col_MSE=MSE(source_continues_data[[col]],index[col])\n",
        "    print(col_MSE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZAp7xHvBG-K"
      },
      "outputs": [],
      "source": [
        "indexs=[MLPm_index_1,MLPm_index_2,MLPm_index_3,MLPm_index_4,MLPm_index_5,MLPm_index_6]\n",
        "\n",
        "for col in columns:\n",
        "  for index in indexs:\n",
        "    col_RMSE=RMSE(source_continues_data[[col]],index[col])\n",
        "    print(col_RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygADWQHiChjV"
      },
      "source": [
        "## 6.2 GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47xlsHWKnR96"
      },
      "outputs": [],
      "source": [
        "unit=\"GRU\"\n",
        "lag = 24\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_1\n",
        "GRUm_index_1={}\n",
        "for col in columns:\n",
        "  Source_missing_final_1['index']=array\n",
        "  Source_missing_final_1=Source_missing_final_1.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_1[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  GRUm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  GRUm.build_Model(unit=\"GRU\")\n",
        "  GRUm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "\n",
        "  GRUm_testPred = GRUm.predict(test_X)\n",
        "  GRUm_testPred = GRUm_testPred.reshape(-1, 1)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  GRUm_testPred = scaler.inverse_transform(GRUm_testPred)\n",
        "  GRUm_testPred=pd.DataFrame(GRUm_testPred).rename(columns={0: col})\n",
        "  GRUm_testPred=GRUm_testPred.iloc[len(GRUm_testPred)-len(index):,:].set_index(index)\n",
        "  GRUm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([GRUm_pre, GRUm_testPred]).sort_index()\n",
        "  GRUm_index_1[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoHaGc3soTrE"
      },
      "outputs": [],
      "source": [
        "unit=\"GRU\"\n",
        "lag = 24\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_2\n",
        "GRUm_index_2={}\n",
        "for col in columns:\n",
        "  Source_missing_final_2['index']=array\n",
        "  Source_missing_final_2=Source_missing_final_2.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_2[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  GRUm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  GRUm.build_Model(unit=\"GRU\")\n",
        "  GRUm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  GRUm_testPred = GRUm.predict(test_X)\n",
        "  GRUm_testPred = GRUm_testPred.reshape(-1, 1)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  GRUm_testPred = scaler.inverse_transform(GRUm_testPred)\n",
        "  GRUm_testPred=pd.DataFrame(GRUm_testPred).rename(columns={0: col})\n",
        "  GRUm_testPred=GRUm_testPred.iloc[len(GRUm_testPred)-len(index):,:].set_index(index)\n",
        "  GRUm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([GRUm_pre, GRUm_testPred]).sort_index()\n",
        "  GRUm_index_2[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g7beiRmoqBq"
      },
      "outputs": [],
      "source": [
        "unit=\"GRU\"\n",
        "lag = 24\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_3\n",
        "GRUm_index_3={}\n",
        "for col in columns:\n",
        "  Source_missing_final_3['index']=array\n",
        "  Source_missing_final_3=Source_missing_final_3.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_3[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  GRUm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  GRUm.build_Model(unit=\"GRU\")\n",
        "  GRUm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  GRUm_testPred = GRUm.predict(test_X)\n",
        "  GRUm_testPred = GRUm_testPred.reshape(-1, 1)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  GRUm_testPred = scaler.inverse_transform(GRUm_testPred)\n",
        "  GRUm_testPred=pd.DataFrame(GRUm_testPred).rename(columns={0: col})\n",
        "  GRUm_testPred=GRUm_testPred.iloc[len(GRUm_testPred)-len(index):,:].set_index(index)\n",
        "  GRUm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([GRUm_pre, GRUm_testPred]).sort_index()\n",
        "  GRUm_index_3[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aAw7toEgo6UH"
      },
      "outputs": [],
      "source": [
        "unit=\"GRU\"\n",
        "lag = 24\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_4\n",
        "GRUm_index_4={}\n",
        "for col in columns:\n",
        "  Source_missing_final_4['index']=array\n",
        "  Source_missing_final_4=Source_missing_final_4.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_4[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  GRUm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  GRUm.build_Model(unit=\"GRU\")\n",
        "  GRUm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  GRUm_testPred = GRUm.predict(test_X)\n",
        "  GRUm_testPred = GRUm_testPred.reshape(-1, 1)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  GRUm_testPred = scaler.inverse_transform(GRUm_testPred)\n",
        "  GRUm_testPred=pd.DataFrame(GRUm_testPred).rename(columns={0: col})\n",
        "  GRUm_testPred=GRUm_testPred.iloc[len(GRUm_testPred)-len(index):,:].set_index(index)\n",
        "  GRUm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([GRUm_pre, GRUm_testPred]).sort_index()\n",
        "  GRUm_index_4[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oANuZzFpCN-"
      },
      "outputs": [],
      "source": [
        "unit=\"GRU\"\n",
        "lag = 24\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_5\n",
        "GRUm_index_5={}\n",
        "for col in columns:\n",
        "  Source_missing_final_5['index']=array\n",
        "  Source_missing_final_5=Source_missing_final_5.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_5[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  GRUm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  GRUm.build_Model(unit=\"GRU\")\n",
        "  GRUm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  GRUm_testPred = GRUm.predict(test_X)\n",
        "  GRUm_testPred = GRUm_testPred.reshape(-1, 1)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  GRUm_testPred = scaler.inverse_transform(GRUm_testPred)\n",
        "  GRUm_testPred=pd.DataFrame(GRUm_testPred).rename(columns={0: col})\n",
        "  GRUm_testPred=GRUm_testPred.iloc[len(GRUm_testPred)-len(index):,:].set_index(index)\n",
        "  GRUm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([GRUm_pre, GRUm_testPred]).sort_index()\n",
        "  GRUm_index_5[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBQjm_dapKio"
      },
      "outputs": [],
      "source": [
        "unit=\"GRU\"\n",
        "lag = 24\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_6\n",
        "GRUm_index_6={}\n",
        "for col in columns:\n",
        "  Source_missing_final_6['index']=array\n",
        "  Source_missing_final_6=Source_missing_final_6.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_6[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  GRUm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  GRUm.build_Model(unit=\"GRU\")\n",
        "  GRUm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  GRUm_testPred = GRUm.predict(test_X)\n",
        "  GRUm_testPred = GRUm_testPred.reshape(-1, 1)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  GRUm_testPred = scaler.inverse_transform(GRUm_testPred)\n",
        "  GRUm_testPred=pd.DataFrame(GRUm_testPred).rename(columns={0: col})\n",
        "  GRUm_testPred=GRUm_testPred.iloc[len(GRUm_testPred)-len(index):,:].set_index(index)\n",
        "  GRUm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([GRUm_pre, GRUm_testPred]).sort_index()\n",
        "  GRUm_index_6[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qd6JMrqk-Me"
      },
      "outputs": [],
      "source": [
        "indexs=[GRUm_index_1,GRUm_index_2,GRUm_index_3,GRUm_index_4,GRUm_index_5,GRUm_index_6]\n",
        "\n",
        "for col in columns:\n",
        "  #print(col)\n",
        "  for index in indexs:\n",
        "    col_MSE=MSE(source_continues_data[[col]],index[col])\n",
        "    print(col_MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMkN9rPdk_ey"
      },
      "outputs": [],
      "source": [
        "indexs=[GRUm_index_1,GRUm_index_2,GRUm_index_3,GRUm_index_4,GRUm_index_5,GRUm_index_6]\n",
        "\n",
        "for col in columns:\n",
        "  for index in indexs:\n",
        "    col_RMSE=RMSE(source_continues_data[[col]],index[col])\n",
        "    print(col_RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX1pTGJ-u7HP"
      },
      "outputs": [],
      "source": [
        "columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODkuoeo7suk8"
      },
      "source": [
        "## 6.6 RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfh4EwyYteaC"
      },
      "outputs": [],
      "source": [
        "unit=\"RNN\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_1\n",
        "RNNm_index_1={}\n",
        "for col in columns:\n",
        "  Source_missing_final_1['index']=array\n",
        "  Source_missing_final_1=Source_missing_final_1.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_1[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  RNNm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  RNNm.build_Model(unit=\"RNN\")\n",
        "  RNNm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  RNNm_train_ModelPred = RNNm.predict(train_X)\n",
        "  RNNm_testPred = RNNm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  RNNm_testPred = scaler.inverse_transform(RNNm_testPred)\n",
        "  RNNm_testPred=pd.DataFrame(RNNm_testPred).rename(columns={0: col})\n",
        "  RNNm_testPred=RNNm_testPred.iloc[len(RNNm_testPred)-len(index):,:].set_index(index)\n",
        "  RNNm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([RNNm_pre, RNNm_testPred]).sort_index()\n",
        "  RNNm_index_1[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjBdsVKHtNrj"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"RNN\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_2\n",
        "RNNm_index_2={}\n",
        "for col in columns:\n",
        "  Source_missing_final_2['index']=array\n",
        "  Source_missing_final_2=Source_missing_final_2.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_2[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  RNNm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  RNNm.build_Model(unit=\"RNN\")\n",
        "  RNNm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  RNNm_train_ModelPred = RNNm.predict(train_X)\n",
        "  RNNm_testPred = RNNm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  RNNm_testPred = scaler.inverse_transform(RNNm_testPred)\n",
        "  RNNm_testPred=pd.DataFrame(RNNm_testPred).rename(columns={0: col})\n",
        "  RNNm_testPred=RNNm_testPred.iloc[len(RNNm_testPred)-len(index):,:].set_index(index)\n",
        "  RNNm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([RNNm_pre, RNNm_testPred]).sort_index()\n",
        "  RNNm_index_2[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhMqnP90v15n"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"RNN\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_3\n",
        "RNNm_index_3={}\n",
        "for col in columns:\n",
        "  Source_missing_final_3['index']=array\n",
        "  Source_missing_final_3=Source_missing_final_3.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_3[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  RNNm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  RNNm.build_Model(unit=\"RNN\")\n",
        "  RNNm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  RNNm_train_ModelPred = RNNm.predict(train_X)\n",
        "  RNNm_testPred = RNNm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  RNNm_testPred = scaler.inverse_transform(RNNm_testPred)\n",
        "  RNNm_testPred=pd.DataFrame(RNNm_testPred).rename(columns={0: col})\n",
        "  RNNm_testPred=RNNm_testPred.iloc[len(RNNm_testPred)-len(index):,:].set_index(index)\n",
        "  RNNm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([RNNm_pre, RNNm_testPred]).sort_index()\n",
        "  RNNm_index_3[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEl8-A-Hv_Cq"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"RNN\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_4\n",
        "RNNm_index_4={}\n",
        "for col in columns:\n",
        "  Source_missing_final_4['index']=array\n",
        "  Source_missing_final_4=Source_missing_final_4.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_4[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  RNNm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  RNNm.build_Model(unit=\"RNN\")\n",
        "  RNNm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  RNNm_train_ModelPred = RNNm.predict(train_X)\n",
        "  RNNm_testPred = RNNm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  RNNm_testPred = scaler.inverse_transform(RNNm_testPred)\n",
        "  RNNm_testPred=pd.DataFrame(RNNm_testPred).rename(columns={0: col})\n",
        "  RNNm_testPred=RNNm_testPred.iloc[len(RNNm_testPred)-len(index):,:].set_index(index)\n",
        "  RNNm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([RNNm_pre, RNNm_testPred]).sort_index()\n",
        "  RNNm_index_4[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPrJ11sfwE2_"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"RNN\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_5\n",
        "RNNm_index_5={}\n",
        "for col in columns:\n",
        "  Source_missing_final_5['index']=array\n",
        "  Source_missing_final_5=Source_missing_final_5.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_5[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  RNNm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  RNNm.build_Model(unit=\"RNN\")\n",
        "  RNNm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  RNNm_train_ModelPred = RNNm.predict(train_X)\n",
        "  RNNm_testPred = RNNm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  RNNm_testPred = scaler.inverse_transform(RNNm_testPred)\n",
        "  RNNm_testPred=pd.DataFrame(RNNm_testPred).rename(columns={0: col})\n",
        "  RNNm_testPred=RNNm_testPred.iloc[len(RNNm_testPred)-len(index):,:].set_index(index)\n",
        "  RNNm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([RNNm_pre, RNNm_testPred]).sort_index()\n",
        "  RNNm_index_5[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSYzF52RwPcs"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"RNN\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_6\n",
        "RNNm_index_6={}\n",
        "for col in columns:\n",
        "  Source_missing_final_6['index']=array\n",
        "  Source_missing_final_6=Source_missing_final_6.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_6[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  RNNm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  RNNm.build_Model(unit=\"RNN\")\n",
        "  RNNm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  RNNm_train_ModelPred = RNNm.predict(train_X)\n",
        "  RNNm_testPred = RNNm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  RNNm_testPred = scaler.inverse_transform(RNNm_testPred)\n",
        "  RNNm_testPred=pd.DataFrame(RNNm_testPred).rename(columns={0: col})\n",
        "  RNNm_testPred=RNNm_testPred.iloc[len(RNNm_testPred)-len(index):,:].set_index(index)\n",
        "  RNNm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([RNNm_pre, RNNm_testPred]).sort_index()\n",
        "  RNNm_index_6[col]=df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGvezCWQtOta"
      },
      "source": [
        "## 6.7 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqx4LAxIwaiK"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"LSTM\"\n",
        "hiddenNum=64\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_1\n",
        "LSTMm_index_1={}\n",
        "for col in columns:\n",
        "  Source_missing_final_1['index']=array\n",
        "  Source_missing_final_1=Source_missing_final_1.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_1[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  LSTMm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  LSTMm.build_Model(unit=\"LSTM\")\n",
        "  LSTMm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  LSTMm_testPred = LSTMm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  LSTMm_testPred = scaler.inverse_transform(LSTMm_testPred)\n",
        "  LSTMm_testPred=pd.DataFrame(LSTMm_testPred).rename(columns={0: col})\n",
        "  LSTMm_testPred=LSTMm_testPred.iloc[len(LSTMm_testPred)-len(index):,:].set_index(index)\n",
        "  LSTMm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([LSTMm_pre, LSTMm_testPred]).sort_index()\n",
        "  LSTMm_index_1[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkWUm4XA6aJ8"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"LSTM\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "hiddenNum=64\n",
        "index=index_2\n",
        "LSTMm_index_2={}\n",
        "for col in columns:\n",
        "  Source_missing_final_2['index']=array\n",
        "  Source_missing_final_2=Source_missing_final_2.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_2[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  LSTMm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  LSTMm.build_Model(unit=\"LSTM\")\n",
        "  LSTMm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  LSTMm_testPred = LSTMm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  LSTMm_testPred = scaler.inverse_transform(LSTMm_testPred)\n",
        "  LSTMm_testPred=pd.DataFrame(LSTMm_testPred).rename(columns={0: col})\n",
        "  LSTMm_testPred=LSTMm_testPred.iloc[len(LSTMm_testPred)-len(index):,:].set_index(index)\n",
        "  LSTMm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([LSTMm_pre, LSTMm_testPred]).sort_index()\n",
        "  LSTMm_index_2[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRLUBBh_6xrs"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"LSTM\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_3\n",
        "LSTMm_index_3={}\n",
        "for col in columns:\n",
        "  Source_missing_final_3['index']=array\n",
        "  Source_missing_final_3=Source_missing_final_3.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_3[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  LSTMm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  LSTMm.build_Model(unit=\"LSTM\")\n",
        "  LSTMm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  LSTMm_train_ModelPred = LSTMm.predict(train_X)\n",
        "  LSTMm_testPred = LSTMm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  LSTMm_testPred = scaler.inverse_transform(LSTMm_testPred)\n",
        "  LSTMm_testPred=pd.DataFrame(LSTMm_testPred).rename(columns={0: col})\n",
        "  LSTMm_testPred=LSTMm_testPred.iloc[len(LSTMm_testPred)-len(index):,:].set_index(index)\n",
        "  LSTMm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([LSTMm_pre, LSTMm_testPred]).sort_index()\n",
        "  LSTMm_index_3[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap5JvRXJ6qRD"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"LSTM\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_4\n",
        "LSTMm_index_4={}\n",
        "for col in columns:\n",
        "  Source_missing_final_4['index']=array\n",
        "  Source_missing_final_4=Source_missing_final_4.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_4[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  LSTMm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  LSTMm.build_Model(unit=\"LSTM\")\n",
        "  LSTMm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  LSTMm_train_ModelPred = LSTMm.predict(train_X)\n",
        "  LSTMm_testPred = LSTMm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  LSTMm_testPred = scaler.inverse_transform(LSTMm_testPred)\n",
        "  LSTMm_testPred=pd.DataFrame(LSTMm_testPred).rename(columns={0: col})\n",
        "  LSTMm_testPred=LSTMm_testPred.iloc[len(LSTMm_testPred)-len(index):,:].set_index(index)\n",
        "  LSTMm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([LSTMm_pre, LSTMm_testPred]).sort_index()\n",
        "  LSTMm_index_4[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJV7kJks67Uw"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"LSTM\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_5\n",
        "LSTMm_index_5={}\n",
        "for col in columns:\n",
        "  Source_missing_final_5['index']=array\n",
        "  Source_missing_final_5=Source_missing_final_5.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_5[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  LSTMm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  LSTMm.build_Model(unit=\"LSTM\")\n",
        "  LSTMm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  LSTMm_train_ModelPred = LSTMm.predict(train_X)\n",
        "  LSTMm_testPred = LSTMm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  LSTMm_testPred = scaler.inverse_transform(LSTMm_testPred)\n",
        "  LSTMm_testPred=pd.DataFrame(LSTMm_testPred).rename(columns={0: col})\n",
        "  LSTMm_testPred=LSTMm_testPred.iloc[len(LSTMm_testPred)-len(index):,:].set_index(index)\n",
        "  LSTMm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([LSTMm_pre, LSTMm_testPred]).sort_index()\n",
        "  LSTMm_index_5[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV0LOuNu7FXD"
      },
      "outputs": [],
      "source": [
        "#RNN\n",
        "unit=\"LSTM\"\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "hidden_dim = 64\n",
        "lr = 1e-4\n",
        "inputDim=1\n",
        "outputDim=1\n",
        "\n",
        "index=index_6\n",
        "LSTMm_index_6={}\n",
        "for col in columns:\n",
        "  Source_missing_final_6['index']=array\n",
        "  Source_missing_final_6=Source_missing_final_6.set_index('index')\n",
        "  dataset_source=source_continues_data[[col]]\n",
        "  dataset_mean=mean_replace(Source_missing_final_6[[col]],col)\n",
        "  ts_mean,data_mean=load_data(dataset_mean,col)\n",
        "  mean_dataset=NormalizeDataset(data_mean)\n",
        "  train_Model_mean,test_mean = dividetrain_ModelTest(mean_dataset, index)\n",
        "  train_Model=train_Model_mean\n",
        "  test=mean_dataset\n",
        "  lookBack=len(train_Model_mean)-len(index)\n",
        "  train_X, trainY = createSamples(train_Model, lookBack)\n",
        "  test_X, testY = createSamples(test, lookBack)\n",
        "  LSTMm=RNNs_M(inputDim, hiddenNum, outputDim, unit, lr)\n",
        "  LSTMm.build_Model(unit=\"LSTM\")\n",
        "  LSTMm.train_Model(train_X, trainY, epoch, batchSize)\n",
        "  LSTMm_train_ModelPred = LSTMm.predict(train_X)\n",
        "  LSTMm_testPred = LSTMm.predict(test_X)\n",
        "  scaler = MinMaxScaler(feature_range=(0.0, 1.0)).fit(dataset_source)\n",
        "  LSTMm_testPred = scaler.inverse_transform(LSTMm_testPred)\n",
        "  LSTMm_testPred=pd.DataFrame(LSTMm_testPred).rename(columns={0: col})\n",
        "  LSTMm_testPred=LSTMm_testPred.iloc[len(LSTMm_testPred)-len(index):,:].set_index(index)\n",
        "  LSTMm_pre=dataset_source.query('index not in @index')\n",
        "  df=pd.concat([LSTMm_pre, LSTMm_testPred]).sort_index()\n",
        "  LSTMm_index_6[col]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRejvQlloAA6"
      },
      "outputs": [],
      "source": [
        "indexs=[LSTMm_index_1,LSTMm_index_2,LSTMm_index_3,LSTMm_index_4,LSTMm_index_5,LSTMm_index_6]\n",
        "\n",
        "for col in columns:\n",
        "  #print(col)\n",
        "  for index in indexs:\n",
        "    col_MSE=MSE(source_continues_data[[col]],index[col])\n",
        "    print(col_MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x90OxbykoBtl"
      },
      "outputs": [],
      "source": [
        "indexs=[LSTMm_index_1,LSTMm_index_2,LSTMm_index_3,LSTMm_index_4,LSTMm_index_5,LSTMm_index_6]\n",
        "\n",
        "for col in columns:\n",
        "  #print(col)\n",
        "  for index in indexs:\n",
        "    col_RMSE=RMSE(source_continues_data[[col]],index[col])\n",
        "    print(col_RMSE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Dissertation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMNwIJUGe+JYRGFAlUrSZeP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}